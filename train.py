# train.py - MindSpore Ascend æœ€ç»ˆå…¼å®¹ç‰ˆ (å·²ä¿®å¤ TotalVariation å…¼å®¹æ€§é”™è¯¯åŠ optimizer å±æ€§é”™è¯¯)
import mindspore as ms
from mindspore import nn, ops, Tensor, context, dtype as mstype
from mindspore.dataset import GeneratorDataset
# å¼•å…¥ T (transforms) ç”¨äº Compose
import mindspore.dataset.transforms as T 
# å¼•å…¥ V (vision) ç”¨äº Resize/CenterCrop
import mindspore.dataset.vision as V
import numpy as np
import random
import time
import os
import cv2

# â— å…³é”®ä¿®æ”¹ 1ï¼šå¼•å…¥å­¦ä¹ ç‡è°ƒåº¦å™¨
from mindspore.nn import cosine_decay_lr 

import transformer
import vgg
import utils

# ------------------ GLOBAL SETTINGS ------------------
TRAIN_IMAGE_SIZE = 256
DATASET_PATH = "train2017"  # è®­ç»ƒæ•°æ®é›†
NUM_EPOCHS = 1
# â— å‚æ•°æ›´æ–°ï¼šæ–°çš„é£æ ¼å›¾åƒè·¯å¾„
STYLE_IMAGE_PATH = "images/oil_painting.jpg"
BATCH_SIZE = 4
# â— ä¼˜åŒ–ï¼šé™ä½å†…å®¹æƒé‡ã€é™ä½æ€»é£æ ¼æƒé‡ã€æé«˜åˆå§‹å­¦ä¹ ç‡ä»¥ç¨³å®š Loss
CONTENT_WEIGHT = 16.0  
STYLE_WEIGHT = 30.0   
ADAM_LR = 2e-4        # (åŸ 1e-3) åˆå§‹å­¦ä¹ ç‡ï¼Œé…åˆåŠ¨æ€è¡°å‡

# â— NEW: TV Loss æƒé‡ (ç”¨äºå¹³æ»‘å›¾åƒï¼Œæ¶ˆé™¤å½©è‰²æ¡çº¹ä¼ªå½±)
TV_WEIGHT = 1e-2 # è¿™æ˜¯ä¸€ä¸ªç»éªŒå€¼ï¼Œç”¨äºè½»å¾®æ­£åˆ™åŒ–å¹³æ»‘åº¦

SAVE_MODEL_PATH = "models/"
SAVE_IMAGE_PATH = "images/results/"
SAVE_MODEL_EVERY = 250
PRINT_GRAD_EVERY = 50
SEED = 35
PLOT_LOSS = 1

# â— å‚æ•°æ›´æ–°ï¼šæ–°çš„å›ºå®šé‡‡æ ·å†…å®¹å›¾è·¯å¾„
FIXED_SAMPLE_CONTENT_PATH = "images/face.jpg" 

# MindSpore Ascend é€‚é…å‚æ•°
GRAD_CLIP_VALUE = 1.0 
NUM_WORKERS = 64 # å¢åŠ æ•°æ®åŠ è½½å¹¶è¡Œæ•°

# ------------------ Device Setting ------------------
target_device = "Ascend"
context.set_context(mode=context.GRAPH_MODE, device_target=target_device)
# ------------------ Seed ------------------
ms.common.set_seed(SEED)
np.random.seed(SEED)
random.seed(SEED)

# ------------------ Dataset ------------------
class CustomDataset:
    def __init__(self, folder_path, transform=None):
        self.folder_path = folder_path
        self.transform = transform
        self.image_paths = [
            os.path.join(folder_path, f) 
            for f in os.listdir(folder_path) 
            if f.lower().endswith(('.jpg', '.jpeg', '.png'))
        ]
        random.shuffle(self.image_paths)

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, index):
        path = self.image_paths[index]
        img = utils.load_image(path)
        if img is None:
            # éšæœºè¿”å›ä¸€ä¸ªæœ‰æ•ˆçš„æ ·æœ¬ï¼Œé¿å…è®­ç»ƒä¸­æ–­
            return self.__getitem__(random.randint(0, len(self.image_paths) - 1))
            
        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        
        if self.transform:
            # è¿™é‡Œçš„ transform æ¥æ”¶ np.ndarray
            img_rgb = self.transform(img_rgb)
            
        # å°† numpy è½¬æ¢æˆ MindSpore Tensorï¼Œå¹¶è½¬åˆ° [-1, 1] èŒƒå›´
        img_tensor = Tensor(img_rgb, mstype.float32)
        # HWC -> CHW (MindSpore/PyTorch æ ¼å¼)
        img_tensor = ops.transpose(img_tensor, (2, 0, 1))
        # å½’ä¸€åŒ–åˆ° [-1, 1]
        img_tensor = img_tensor / 127.5 - 1.0
        
        return img_tensor

def create_dataloader(dataset_path, image_size, batch_size, num_workers):
    # ä¿®æ­£ï¼šä½¿ç”¨ T.Compose æ›¿æ¢ V.Compose
    transform = T.Compose([
        V.Resize(image_size),
        V.CenterCrop(image_size)
    ])
    
    dataset = CustomDataset(dataset_path, transform=transform)
    
    # ç¡®ä¿ drop_remainder=True ä»¥åœ¨ GRAPH_MODE ä¸‹ä¿æŒå›ºå®šçš„ Batch Size
    data_loader = GeneratorDataset(
        dataset, 
        column_names=["content_image"],
        shuffle=True, 
        num_parallel_workers=num_workers,
        max_rowsize=32  # å¢åŠ  max_rowsize é¿å…å†…å­˜è­¦å‘Š
    )
    data_loader = data_loader.batch(batch_size, drop_remainder=True)
    # â— å…³é”®ä¿®æ”¹ï¼šè¿”å› dataset å¯¹è±¡
    return data_loader, len(dataset), dataset 

# ------------------ Loss Network (æ ¸å¿ƒä¿®æ”¹) ------------------
class StyleTransferLoss(nn.Cell):
    def __init__(self, transformer_net, content_weight, style_weight, tv_weight):
        super().__init__()
        self.transformer = transformer_net
        self.vgg = vgg.VGG19_Feature()
        
        self.content_weight = content_weight
        self.style_weight = style_weight
        self.tv_weight = tv_weight 

        # â— ä¼˜åŒ–ï¼šæ–°çš„åˆ†å±‚é£æ ¼æƒé‡ (Style Layer Weights) - ä¿æŒç”¨æˆ·å½“å‰é…ç½®
        self.style_layer_weights = {
            'relu1_2': 0.5, 
            'relu2_2': 1.0,
            'relu3_4': 1.0,
            'relu4_4': 0.8,
            'relu5_4': 0.2  
        }
        
        self.square = ops.Square()
        self.reduce_mean = ops.ReduceMean()
        
        # ä¿®å¤ï¼šç”¨äºæ‰‹åŠ¨è®¡ç®— TV Loss
        self.abs = ops.Abs()
        self.reduce_sum = ops.ReduceSum()

        # é¢„å…ˆè®¡ç®—é£æ ¼å›¾åƒçš„ç‰¹å¾å’Œ Gram çŸ©é˜µ
        style_image = utils.load_image(STYLE_IMAGE_PATH)
        
        # â— æ ¸å¿ƒä¿®æ”¹ï¼šCLAHE é¢„å¤„ç†é£æ ¼å›¾ï¼Œå¢å¼ºé«˜äº®/é«˜æš—é²æ£’æ€§
        if style_image is None:
             raise FileNotFoundError(f"âŒ ä¸¥é‡é”™è¯¯: æ— æ³•åŠ è½½é£æ ¼å›¾åƒï¼Œè¯·æ£€æŸ¥è·¯å¾„: {STYLE_IMAGE_PATH}")
        
        # 1. ç¼©æ”¾é£æ ¼å›¾ï¼Œé¿å… MindSpore å†…å­˜æº¢å‡º (ä¿æŒåŸæœ‰é€»è¾‘)
        TARGET_STYLE_SIZE = 720 
        h, w = style_image.shape[:2]
        if max(h, w) > TARGET_STYLE_SIZE:
            ratio = TARGET_STYLE_SIZE / max(h, w)
            new_h = int(h * ratio)
            new_w = int(w * ratio)
            style_image = cv2.resize(style_image, (new_w, new_h), interpolation=cv2.INTER_AREA)
            print(f"âœ… é£æ ¼å›¾å·²ç¼©æ”¾è‡³: {new_w}x{new_h}ï¼Œä»¥é¿å… MindSpore å†…å­˜æº¢å‡ºã€‚")
            
        # 2. BGR -> RGBï¼Œå‡†å¤‡è¿›è¡Œ CLAHE
        style_image_rgb = cv2.cvtColor(style_image, cv2.COLOR_BGR2RGB)
        
        # 3. CLAHE (Contrast Limited Adaptive Histogram Equalization) 
        style_img_yuv = cv2.cvtColor(style_image_rgb, cv2.COLOR_RGB2YUV)
        y, u, v = cv2.split(style_img_yuv)
        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
        y_clahe = clahe.apply(y)
        yuv_clahe = cv2.merge([y_clahe, u, v])
        style_img_clahe = cv2.cvtColor(yuv_clahe, cv2.COLOR_YUV2RGB)
        
        # 4. è½¬æ¢åˆ° Tensor
        style_img_clahe = style_img_clahe.astype(np.float32)
        style_img_clahe = style_img_clahe / 255.0 * 2.0 - 1.0 
        style_tensor = Tensor(np.expand_dims(style_img_clahe.transpose(2, 0, 1), 0), mstype.float32)
        
        self.style_features = self.vgg(style_tensor) 
        
        self.style_grams = {}
        for name, feature in self.style_features.items():
            gram_matrix = utils.gram(feature)
            self.style_grams[name] = ops.squeeze(gram_matrix, axis=0) 
            
    def _mse_loss(self, pred, target):
        """æ‰‹åŠ¨è®¡ç®— MSE lossï¼šMean(Square(Pred - Target))"""
        return self.reduce_mean(self.square(pred - target))

    def construct(self, content_image):
        generated_image = self.transformer(content_image)
        
        # 1. Content Loss
        content_features = self.vgg(content_image)
        generated_features = self.vgg(generated_image)
        
        # Content Loss Layer: relu3_4
        content_loss = self._mse_loss(content_features['relu2_2'], generated_features['relu2_2'])
        
        # 2. Style Loss
        style_loss = Tensor(0.0, mstype.float32)
        for layer in ['relu1_2','relu2_2','relu3_4','relu4_4','relu5_4']:
            gen_gram = utils.gram(generated_features[layer])
            style_gram = self.style_grams[layer]
            C = style_gram.shape[0] 
            broadcast_shape = (gen_gram.shape[0], C, C) 
            style_gram_batched = ops.broadcast_to(style_gram, broadcast_shape)
            
            # ç´¯åŠ  Style Loss æ—¶åº”ç”¨åˆ†å±‚æƒé‡
            layer_loss = self._mse_loss(gen_gram, style_gram_batched)
            style_loss += layer_loss * self.style_layer_weights[layer] 

        # â— æ‰‹åŠ¨è®¡ç®— TV Loss (L1 norm Total Variation)
        # è®¡ç®—é«˜æ–¹å‘çš„å·®å€¼ (H, W-1)
        tv_loss_h = self.abs(generated_image[:, :, 1:, :] - generated_image[:, :, :-1, :])
        # è®¡ç®—å®½æ–¹å‘çš„å·®å€¼ (H-1, W)
        tv_loss_w = self.abs(generated_image[:, :, :, 1:] - generated_image[:, :, :, :-1])
        # å°†æ‰€æœ‰å·®å€¼çš„ç»å¯¹å€¼æ±‚å’Œï¼Œä½œä¸º TV Loss
        tv_loss = self.reduce_sum(tv_loss_h) + self.reduce_sum(tv_loss_w)

        # 3. Total Loss (åŠ å…¥ TV Loss)
        total_loss = (self.content_weight * content_loss + 
                      self.style_weight * style_loss +
                      self.tv_weight * tv_loss)
        
        # 4. è¿”å› tv_loss
        return total_loss, content_loss, style_loss, tv_loss, generated_image

# ------------------ Training ------------------

# â— å…³é”®æ–°å¢ï¼šå›ºå®šé‡‡æ ·å›¾é¢„å¤„ç†å‡½æ•°
def _load_and_preprocess_sample_image(path, size):
    """åŠ è½½å¹¶é¢„å¤„ç†å›ºå®šé‡‡æ ·å›¾ï¼Œä½¿å…¶ä¸ dataloader è¾“å‡ºæ ¼å¼ä¸€è‡´ï¼š[1, C, H, W], [-1, 1]"""
    # 1. åŠ è½½å›¾åƒ
    raw_sample_img = utils.load_image(path)
    if raw_sample_img is None:
        raise FileNotFoundError(f"æ— æ³•åŠ è½½é‡‡æ ·å†…å®¹å›¾ï¼Œè¯·æ£€æŸ¥è·¯å¾„: {path}")

    # 2. BGR -> RGB
    img_rgb = cv2.cvtColor(raw_sample_img, cv2.COLOR_BGR2RGB)
    
    # 3. ä»¿ç…§ dataloader çš„ Resize/CenterCrop
    h, w, _ = img_rgb.shape
    ratio = size / min(h, w)
    new_w = int(w * ratio)
    new_h = int(h * ratio)
    
    img_resized = cv2.resize(img_rgb, (new_w, new_h), interpolation=cv2.INTER_LINEAR)
    
    start_h = (new_h - size) // 2
    start_w = (new_w - size) // 2
    img_cropped = img_resized[start_h:start_h + size, start_w:start_w + size, :]
    
    # 4. HWC -> CHW, Normalize to [-1, 1], Expand_dims
    fixed_sample_tensor = Tensor(img_cropped, mstype.float32)
    fixed_sample_tensor = ops.transpose(fixed_sample_tensor, (2, 0, 1)) # CHW
    fixed_sample_tensor = fixed_sample_tensor / 127.5 - 1.0
    fixed_sample_tensor = ops.expand_dims(fixed_sample_tensor, 0) # [1, C, H, W]
    
    return fixed_sample_tensor


def train():
    # 1. Prepare Data
    data_loader, total_data_size, raw_dataset = create_dataloader(DATASET_PATH, TRAIN_IMAGE_SIZE, BATCH_SIZE, NUM_WORKERS)
    steps_per_epoch = data_loader.get_dataset_size()
    print(f"Dataset Size: {total_data_size}")

    # è®¡ç®—æ€»è®­ç»ƒæ­¥æ•°
    TOTAL_TRAIN_STEPS = steps_per_epoch * NUM_EPOCHS
    print(f"Total Training Steps: {TOTAL_TRAIN_STEPS}")

    # 2. Prepare Fixed Sample Image
    sample_img_path = FIXED_SAMPLE_CONTENT_PATH
    
    if not os.path.exists(sample_img_path) and raw_dataset.image_paths:
         sample_img_path = raw_dataset.image_paths[0] 
         print(f"âš ï¸ è­¦å‘Š: æœªæ‰¾åˆ°å›ºå®šçš„é‡‡æ ·å†…å®¹å›¾ã€‚ä½¿ç”¨æ•°æ®é›†ä¸­çš„ç¬¬ä¸€å¼ å›¾ä½œä¸ºé‡‡æ ·å›¾: {sample_img_path}")
    
    try:
        fixed_sample_tensor = _load_and_preprocess_sample_image(sample_img_path, TRAIN_IMAGE_SIZE)
        print(f"âœ… å·²åŠ è½½å›ºå®šé‡‡æ ·å†…å®¹å›¾: {os.path.basename(sample_img_path)}")
    except FileNotFoundError as e:
         print(f"âŒ ä¸¥é‡é”™è¯¯: {e}")
         return
    
    # 3. Network and Loss
    global TransformerNetwork
    TransformerNetwork = transformer.TransformerNet(high_res_mode=False) 
    
    LossNetwork = StyleTransferLoss(TransformerNetwork, CONTENT_WEIGHT, STYLE_WEIGHT, TV_WEIGHT)
    LossNetwork.set_train() 
    
    # 4. Optimizer and Training Cell 
    lr = cosine_decay_lr(
        min_lr=0.0,
        max_lr=ADAM_LR,
        total_step=TOTAL_TRAIN_STEPS,
        step_per_epoch=steps_per_epoch,
        decay_epoch=NUM_EPOCHS
    )
    
    optimizer = nn.Adam(TransformerNetwork.trainable_params(), learning_rate=lr) 
    
    class TrainOneStepCell(nn.Cell):
        def __init__(self, net, optimizer, grad_clip):
            super().__init__()
            self.net = net
            self.optimizer = optimizer 
            self.grad_fn = ops.value_and_grad(self.net, None, self.optimizer.parameters, has_aux=True) 
            self.clip_by_norm = nn.ClipByNorm(axis=None)
            self.hyper_map = ops.HyperMap()
            grad_clip_tensor = ops.scalar_to_tensor(grad_clip)
            num_params = len(self.optimizer.parameters)
            self.clip_norm_tensors = tuple([grad_clip_tensor] * num_params)

        def construct(self, content_image):
            (total_loss, content_loss, style_loss, tv_loss, generated_image), grads = self.grad_fn(content_image)
            grads = self.hyper_map(self.clip_by_norm, grads, self.clip_norm_tensors)
            total_loss = ops.depend(total_loss, self.optimizer(grads))
            return total_loss, content_loss, style_loss, tv_loss, generated_image

    train_net = TrainOneStepCell(LossNetwork, optimizer, GRAD_CLIP_VALUE)
    
    # 5. Start Training
    start_time = time.time()
    batch_count = 0
    
    # --- åˆå§‹åŒ–æ‰€æœ‰ Loss å†å²è®°å½•åˆ—è¡¨ ---
    total_loss_history = []
    content_loss_history = []
    style_loss_history = []
    tv_loss_history = [] # NEW
    
    for epoch in range(NUM_EPOCHS):
        for step, (content_batch,) in enumerate(data_loader.create_tuple_iterator()):
            current_total, current_content, current_style, current_tv, generated_batch = train_net(content_batch)
            
            # è½¬æ¢ä¸º numpy æ ¼å¼ç”¨äºè®°å½•å’Œç»˜å›¾
            current_total = current_total.asnumpy()
            current_content = current_content.asnumpy()
            current_style = current_style.asnumpy()
            current_tv = current_tv.asnumpy()
            
            batch_count += 1
            batch_in_epoch = step + 1

            # è®°å½•æ•°æ®
            total_loss_history.append(current_total)
            content_loss_history.append(current_content)
            style_loss_history.append(current_style)
            tv_loss_history.append(current_tv) # NEW

            # æ‰“å°æ—¥å¿—
            if batch_count % PRINT_GRAD_EVERY == 0:
                print(f"Epoch [{epoch+1}/{NUM_EPOCHS}] Step [{batch_in_epoch}/{steps_per_epoch}] "
                      f"Loss: {current_total:.2f} (Content: {current_content:.2f}, Style: {current_style:.2f}, TV: {current_tv:.6f})")
            
            # æ¯ 250 æ­¥ï¼ˆSAVE_MODEL_EVERYï¼‰æ‰§è¡Œï¼šä¿å­˜æ¨¡å‹ã€é‡‡æ ·ã€ç»˜å›¾
            if batch_count % SAVE_MODEL_EVERY == 0 or (epoch == NUM_EPOCHS - 1 and step == steps_per_epoch - 1):
                os.makedirs(SAVE_MODEL_PATH, exist_ok=True)
                os.makedirs(SAVE_IMAGE_PATH, exist_ok=True)
                
                # A. ä¿å­˜æ¨¡å‹
                checkpoint_path = os.path.join(SAVE_MODEL_PATH, f"checkpoint_{batch_count}.ckpt")
                ms.save_checkpoint(TransformerNetwork, checkpoint_path)
                
                # B. å…³é”®ä¿®æ”¹ï¼šå®æ—¶ç»˜åˆ¶å¹¶ä¿å­˜æ‰€æœ‰ Loss æ›²çº¿
                if PLOT_LOSS:
                    plot_save_path = os.path.join(SAVE_IMAGE_PATH, "loss_curve_latest.png")
                    utils.plot_losses(
                        total_loss_history, 
                        content_loss_history, 
                        style_loss_history, 
                        tv_loss_history, # ä¼ å…¥å››ä¸ªå‚æ•°
                        save_path=plot_save_path
                    )
                
                # C. ä½¿ç”¨å›ºå®šé‡‡æ ·å›¾è¿›è¡Œæ¨ç†
                TransformerNetwork.set_train(False) 
                sample_tensor = TransformerNetwork(fixed_sample_tensor) 
                TransformerNetwork.set_train(True) 
                
                # ä¿å­˜é‡‡æ ·å›¾åƒ
                sample_image = utils.ttoi(sample_tensor)
                utils.saveimg(sample_image, os.path.join(SAVE_IMAGE_PATH, f"sample_fixed_{batch_count}.png"))
                print(f"ğŸ“Š Step {batch_count}: æ¨¡å‹å·²ä¿å­˜ï¼ŒLoss æ›²çº¿å·²æ›´æ–°ã€‚")

    stop_time = time.time()
    print(f"Done Training! Time elapsed: {stop_time - start_time:.2f} seconds")
    
    # Final Save
    TransformerNetwork.set_train(False)
    final_path = os.path.join(SAVE_MODEL_PATH, f"final_{os.path.basename(STYLE_IMAGE_PATH).split('.')[0]}.ckpt")
    ms.save_checkpoint(TransformerNetwork, final_path)
    print(f"Final checkpoint saved to {final_path}")


if __name__ == "__main__":
    train()